_Note: Source document was split into 2 OCR chunks (pages 1-12, pages 13-15) to stay within token limits._

# 2025-03-11_DAV_EB_AI_Act_im_aktuariellen_Kontext

## Page 1
Ergebnisbericht des Ausschusses Actuarial Data Science

# Der Artificial Intelligence Act im aktuariellen Kontext 

Köln, 11. März 2025
![Page 1 Image 1](2025_03_11_dav_eb_ai_act_im_aktuariellen_kontext_assets/2025_03_11_dav_eb_ai_act_im_aktuariellen_kontext_p01_img1.jpg)

## Page 2
# Präambel 

Die Arbeitsgruppe Artificial Intelligence Act des Ausschusses Actuarial Data Science der Deutschen Aktuarvereinigung e. V. (DAV) hat den vorliegenden Ergebnisbericht erstellt. ${ }^{1}$

## Zusammenfassung und Anwendungsbereich

Im vorliegenden Bericht werden die Auswirkungen des Artificial Intelligence Act (AI Act), also der KI-Verordnung [1], der Europäischen Union (EU) auf die Versicherungsbranche und die Arbeit von Aktuarinnen und Aktuaren analysiert. Der AI Act, der am 01. August 2024 in Kraft trat, ist Teil der EU-Digitalstrategie und soll den sicheren, transparenten und ethischen Einsatz von KI-Systemen sicherstellen.

Die Verordnung kategorisiert KI-Anwendungen nach ihrem Risikopotential in vier Kategorien:

- verbotene Praktiken
- Hochrisikosysteme
- KI-Systeme mit besonderen Transparenzanforderungen
- alle weiteren KI-Systeme

Speziell die als Hochrisikosysteme kategorisierten Anwendungen, wie sie auch in der Versicherungsbranche verwendet werden, unterliegen strengen Anforderungen. Dazu zählen unter anderem Anforderungen im Bereich Transparenz, Datenqualität und kontinuierliche Überwachung. Insbesondere Unternehmen der Lebens- und Krankenversicherung sind hiervon in gewissen Anwendungsfällen betroffen. Konkret betrifft dies
„KI-Systeme, die bestimmungsgemäß für die Risikobewertung und Preisbildung in Bezug auf natürliche Personen im Fall von Kranken- und Lebensversicherungen verwendet werden sollen;" ([1], Anhang III, Punkt 5c).

Versicherungsunternehmen müssen sicherstellen, dass ihre KI-Systeme den gesetzlichen Anforderungen entsprechen, was umfassende Dokumentation, hohe Datenqualität und kontinuierliche Überwachung beinhaltet. Aktuarinnen und Aktuare ${ }^{2}$ spielen dabei eine zentrale Rolle, da sie die Modelle und Algorithmen entwickeln, die den regulatorischen Anforderungen gerecht werden müssen. Der Bericht betont, dass die Einhaltung der neuen Vorschriften nicht nur zur gesetzlichen Konformität beiträgt, sondern auch das Vertrauen der Kunden stärkt und die Wettbewerbsfähigkeit der Unternehmen erhöht. Der AI Act bietet somit eine Gelegenheit durch den verantwortungsvollen und innovativen Einsatz von KI das Vertrauen der Kunden in die Versicherungsbranche zu stärken.

Der Ergebnisbericht ist an die Mitglieder und Gremien der DAV zur Information über den Stand der Diskussion und die erzielten Erkenntnisse gerichtet und stellt keine berufsständisch legitimierte Position der DAV dar. ${ }^{3}$

[^0]
[^0]:    ${ }^{1}$ Der Ausschuss dankt der Arbeitsgruppe Artificial Intelligence Act ausdrücklich für die geleistete Arbeit, namentlich Dr. Stefan Nörtemann (Leitung), Prof. Dr. Jonas Offtermatt, Teresa Reichart und Dr. Michael Zimmer.
    ${ }^{2}$ Auch wenn hier und im Folgenden die Aktuarinnen und Aktuare explizit genannt werden, spricht die DAV alle Geschlechter und Identitäten gleichermaßen an. Dies gilt auch für alle anderen hier genannten Personengruppen.
    ${ }^{3}$ Die sachgemäße Anwendung des Ergebnisberichts erfordert aktuarielle Fachkenntnisse. Dieser Ergebnisbericht stellt deshalb keinen Ersatz für entsprechende professionelle aktuarielle Dienstleistungen dar. Aktuarielle Entscheidungen mit Auswirkungen auf persönliche Vorsorge und Absicherung, Kapitalanlage oder geschäftliche Aktivitäten sollten ausschließlich auf Basis der Beurteilung durch eine(n) qualifizierte(n) Aktuar DAV/Aktuarin DAV getroffen werden.

## Page 3
# Schlagworte 

Künstliche Intelligenz, KI-Verordnung, Artificial Intelligence Act, KI-Regulierung, ethische KI, Hochrisikosysteme, KI-Modelle mit allgemeinem Verwendungszweck, general-purpose AI models

## Verabschiedung

Dieser Ergebnisbericht ist durch den Ausschuss Actuarial Data Science am 11. März 2025 verabschiedet worden.

## Page 4
This abstract summarises the report on findings "Der Artificial Intelligence Act im aktuariellen Kontext", which was approved by the DAV committee Actuarial Data Science on 11.03.2025.

# The Artificial Intelligence Act in an actuarial context 

This report analyzes the impact of the Artificial Intelligence Act (AI Act), i.e. the AI Regulation [1], of the European Union (EU) on the insurance industry and the work of actuaries. The AI Act, which came into force on August 1, 2024, is part of the EU's digital strategy and is intended to ensure the safe, transparent and ethical use of AI systems.

The regulation categorizes AI applications into four categories according to their risk potential

- prohibited practices
- high-risk systems
- AI systems with special transparency requirements
- all other AI systems

In particular, applications categorized as high-risk systems, such as those used in the insurance industry, are subject to strict requirements. These include requirements in the areas of transparency, data quality and continuous monitoring. Life and health insurance companies in particular are affected by this in certain application cases. Specifically, this concerns
"AI systems intended to be used for risk assessment and pricing in relation to natural persons in the case of life and health insurance;" ([1], Annex III, point 5c).

Insurance companies must ensure that their AI systems comply with legal requirements, which includes comprehensive documentation, high data quality and continuous monitoring. Actuaries play a central role in this, as they develop the models and algorithms that must meet the regulatory requirements. The report emphasizes that compliance with the new regulations not only contributes to legal conformity, but also strengthens customer confidence and increases the competitiveness of companies. The AI Act thus offers an opportunity to strengthen customer trust in the insurance industry through the responsible and innovative use of AI.

Reports on findings are summaries of the results of work carried out by DAV committees or working groups,

- where their application can be freely decided upon within the framework of the code of conduct,
- that should inform discussion of the current opinion among actuaries or also among the broader public.

As working results of a single committee, they do not, for the time being, represent any recognised position within the DAV and do not comprise any actuarial standards of practice. In this respect they are clearly distinguishable from any standards of practice.

## Page 5
# Inhaltsverzeichnis 

1. Einleitung - Der Artificial Intelligence Act ..... 6
1.1. Zielsetzung ..... 6
1.2. Einordnung in den bestehenden Rechtsrahmen ..... 6
1.3. Governance ..... 6
2. Regulatorik - Überblick ..... 7
2.1. Definitionen ..... 7
2.2. Regelungssystematik ..... 7
2.2.1. Verbotene Praktiken ..... 8
2.2.2. Hochrisikosysteme ..... 8
2.2.3. KI-Systeme mit besonderen Transparenzanforderungen ..... 9
2.2.4. Alle weiteren KI-Anwendungen ..... 10
2.3. Sanktionen ..... 10
2.4. Inkraftreten und Geltungsbeginn ..... 10
3. Auswirkungen ..... 11
3.1. Auswirkungen auf Versicherungen ..... 11
3.2. Auswirkungen für Aktuarinnen und Aktuare ..... 12
3.3. Risikobewertung einer konkreten KI-Anwendung gemäß KI-Verordnung - Checkliste ..... 13
4. Literaturverzeichnis ..... 15

## Page 6
# 1. Einleitung - Der Artificial Intelligence Act 

Künstliche Intelligenz (KI) hat in den letzten Jahren eine immer zentralere Rolle in der modernen Wirtschaft und Gesellschaft eingenommen. Auch in der Versicherungs- und Finanzbranche bietet KI ein enormes Potenzial zur Optimierung von Risikobewertungen, Schadensbeurteilungen, Betrugserkennung und anderen Anwendungsfällen. Um den sicheren und ethischen Einsatz von KI zu gewährleisten, hat die Europäische Union eine KI-Verordnung, den AI Act [1], entwickelt und am 21. Mai 2024 offiziell verabschiedet. Die meisten Teile der Verordnung gelten 24 Monate nach Inkrafttreten, konkret ab August 2026. Innerhalb dieses Ergebnisberichtes werden die grundlegen Regelungen sowie Auswirkungen dieser auf die Versicherungsbranche und das Aktuariat vorgestellt.

### 1.1. Zielsetzung

Mit dem AI Act soll ein harmonisierter Rechtsrahmen für die Entwicklung und Anwendung von KI in der EU geschaffen werden, der sowohl den Schutz der Grundrechte und -freiheiten der Bürgerinnen und Bürger als auch die Förderung von Innovationen gewährleistet. Zu diesem Zweck werden Regeln für die Nutzung und den Verkauf von KI festgelegt und bestimmte KI-Praktiken gänzlich verboten. KI-Systeme werden in verschiedene Risikoklassen (geringes bis hohes Risiko) eingeteilt, für die dann unterschiedlich strenge Anforderungen gelten. Ähnlich wie bei der Daten-schutz-Grundverordnung gilt für die Anwendung das Marktortprinzip. Das bedeutet, dass die Verordnung für alle gilt, die innerhalb der EU KI-Systeme herstellen, verwenden, importieren oder vertreiben, unabhängig davon, wo sie ihren Sitz haben.

### 1.2. Einordnung in den bestehenden Rechtsrahmen

Der Artificial Intelligence Act (AI Act) ist ein wesentlicher Bestandteil der EU-Digitalstrategie, die im Rahmen der politischen Leitlinien der Kommission von Ursula von der Leyen für 2019-2024 entwickelt wurde. Zu den zentralen Verordnungen dieser Strategie gehören der Data Governance Act, der seit November 2023 in Kraft ist und die Verfügbarkeit von Daten in der EU regelt, der Digital Services Act, der seit Februar 2024 einen sicheren digitalen Raum schafft, und der Digital Markets Act, der seit Mai 2023 gegen unfaire Praktiken großer Online-Plattformen vorgeht. Ergänzt wird dies durch den Data Act, der ab September 2025 den Zugang zu Daten erleichtern soll. Zusammen mit dem AI Act bilden diese Verordnungen das Fundament der EU-Digitalstrategie und verdeutlichen den Anspruch der EU, eine führende Rolle in der globalen digitalen Entwicklung einzunehmen.

### 1.3. Governance

„Mit dem AI Act sollte ein Governance-Rahmen geschaffen werden, der sowohl die Koordinierung und Unterstützung der Anwendung dieser Verordnung auf nationaler Ebene als auch den Aufbau von Kapazitäten auf Unionsebene und die Integration von Interessenträgern im Bereich der KI ermöglicht. Für die wirksame Umsetzung und Durchsetzung dieser Verordnung ist ein GovernanceRahmen erforderlich, der es ermöglicht, zentrales Fachwissen auf Unionsebene zu koordinieren und aufzubauen. Per Kommissionbeschluss (45) wurde das Büro für Künstliche Intelligenz errichtet, dessen Aufgabe es ist, Fachwissen und Kapazitäten der Union im Bereich der KI zu entwickeln und zur Umsetzung des Unionsrechts im KI-Bereich beizutragen" (aus [1], Erwägungsgrund 148 \& Artikel 64).

Die Kommission entwickelt über das Büro für Künstliche Intelligenz („AI Office") die Sachkenntnis und Fähigkeiten der Union auf dem Gebiet der KI (Artikel 64). Das Büro für künstliche Intelligenz spielt eine zentrale Rolle bei der Umsetzung und Überwachung der KI-Verordnung. Eine seiner Hauptaufgaben wird es sein, die Einhaltung der Vorschriften durch Unternehmen und Organisationen zu unterstützen und sicherzustellen.

Neben dem Büro für künstliche Intelligenz („AI Office") wird ein Europäisches Gremium für Künstliche Intelligenz („KI-Gremium"), ein Beratungsforum („advisory forum") und ein Wissenschaftliches Gremium unabhängiger Sachverständiger eingerichtet.

## Page 7
Das KI-Gremium setzt sich aus einem Vertreter je Mitgliedstaat zusammen (Artikel 65). Es berät und unterstützt die Kommission und die Mitgliedstaaten, um die einheitliche und wirksame Anwendung dieser Verordnung zu erleichtern (Artikel 66).

Das Beratungsforum stellt technisches Fachwissen bereit und berät das KI-Gremium und die Kommission bei deren Aufgaben. Die Kommission ernennt die Mitglieder des Beratungsforums aus dem Kreis der Interessenträger mit anerkanntem Fachwissen auf dem Gebiet der KI und achtet auf eine ausgewogene Auswahl von Interessenträgern, darunter die Industrie, Start-up-Unternehmen, kleinere und mittlere Unternehmen, die Zivilgesellschaft und die Wissenschaft (Artikel 67).

Das wissenschaftliche Gremium setzt sich aus Sachverständigen zusammen, die von der Kommission auf der Grundlage aktueller wissenschaftlicher oder technischer Fachkenntnisse auf dem Gebiet der KI ausgewählt werden (Artikel 68). Das wissenschaftliche Gremium berät und unterstützt das Büro für Künstliche Intelligenz wissenschaftlich fundiert und unabhängig bei seinen Aufgaben.

# 2. Regulatorik - Überblick 

Im Folgenden werden die grundlegenden Regelungen der Verordnung kurz zusammengefasst und erläutert. Dies kann aufgrund des Umfangs der Verordnung nur überblicksartig erfolgen.

### 2.1. Definitionen

Innerhalb der Verordnung werden einige zentrale Begriffe im Themenfeld der Künstlichen Intelligenz erstmalig gesetzgeberisch definiert, bspw. Trainingsdaten, Validierungsdaten, Testdaten und Deep Fake. Aber es wird auch grundsätzlich festgelegt, was unter einem KI-System zu verstehen ist. In Artikel 3 heißt es:
"KI-System": ein maschinengestütztes System, das so konzipiert ist, dass es mit unterschiedlichem Grad an Autonomie operieren kann und nach dem Einsatz Anpassungsfähigkeit zeigen kann, und das für explizite oder implizite Ziele aus den Eingaben, die es erhält, ableitet, wie es Ergebnisse wie Vorhersagen, Inhalte, Empfehlungen oder Entscheidungen erzeugen kann, die physische oder virtuelle Umgebungen beeinflussen können.

Insbesondere die Formulierung „aus den Eingaben, [..] Ergebnisse wie Vorhersagen, Empfehlungen oder Entscheidungen erzeugen kann [..]" führt dazu, dass viele Anwendungsfälle im Versicherungsbereich als KI-Systeme im Sinne der Verordnung interpretiert werden müssen. Damit gelten für diese auch die folgenden Regelungen.

### 2.2. Regelungssystematik

Die Anforderungen an KI-Systeme beruhen weitgehend auf den Empfehlungen der High-Level-Expert-Group (HLEG), die im Auftrag der Europäischen Kommission ethische Leitlinien für vertrauenswürdige KI formuliert hat [2]. Während die HLEG einen holistischen Ansatz verfolgt und Anforderungen im Sinne einer vertrauenswürdigen KI für alle KI-Anwendungen formuliert, verfolgt die EU in ihrer Verordnung einen risikobasierten Ansatz.

Die Anforderungen, die nach dem AI Act für ein KI-System gelten, sind abhängig von dessen Risikopotential. Dabei werden Risikokategorien von unannehmbarem Risiko über Hochrisiko bis hin zu minimalem Risiko unterschieden. Konkret werden vier Kategorien definiert:

- verbotene Praktiken
- Hochrisikosysteme
- KI-Systeme mit besonderen Transparenzanforderungen
- alle weiteren KI-Systeme

## Page 8
Zusätzlich wird zwischen verschiedenen Akteuren unterschieden. Gemäß der Begriffsdefinitionen in Artikel 3 werden Anbieter, Produkthersteller, Betreiber, Bevollmächtigten ${ }^{4}$, Einführer und Händler als Akteure definiert, für die ggf. unterschiedliche Anforderungen definiert sind. Dabei ist zu beachten, dass Unternehmen und/oder Personen auch mehrere Rollen in von der Regulierung betroffen sein kann. Entwickelt ein Softwareunternehmen eine KI-Anwendung und bietet diese am Markt an, so hat es die Rollen Produkthersteller und Anbieter. Betreibt das Unternehmen die KIAnwendung in eigener Verantwortung in einer Cloud, so hat es zusätzlich die Rolle Betreiber.

In der Praxis, etwa bei den Anforderungen an Hochrisikosysteme, wird zwischen Anwendern und Anbietern unterschieden.

# 2.2.1. Verbotene Praktiken 

In Artikel 5 werden Praktiken, die ein unannehmbares Risiko darstellen, innerhalb der EU grundsätzlich verboten. Dies betrifft unter anderem KI-Anwendungen zur unterschwelligen Beeinflussung, um das Verhalten einer Person zu beeinflussen oder zur Ausnutzung einer Schwäche oder Schutzbedürftigkeit einer Person aufgrund ihres Alters oder ihrer körperlichen oder geistigen Behinderung, die dieser oder einer anderen Person einen physischen oder psychischen Schaden zufügt.

Zudem sind KI-Anwendungen zur Bewertung natürlicher Personen auf Grundlage ihres Sozialverhaltens oder vorhergesagter Persönlichkeitsmerkmale verboten, sofern dies zu einer Schlechterstellung oder Benachteiligung führt.

Ebenfalls verboten ist die Verwendung biometrischer Echtzeit-Fernidentifizierungssysteme in öffentlich zugänglichen Räumen zu Strafverfolgungszwecken, wobei hier diverse Ausnahmen formuliert sind.

### 2.2.2. Hochrisikosysteme

Hochrisikosysteme werden in Artikel 6 definiert und in Anhang III werden verschiedene Anwendungsfelder aufgelistet. Hier werden unter anderem die Verwaltung und der Betrieb kritischer Infrastrukturen, die allgemeine und berufliche Bildung, Beschäftigung, Personalmanagement und Zugang zur Selbstständigkeit, Strafverfolgung, Migration, Asyl und Grenzkontrolle sowie Rechtspflege und demokratische Prozesse genannt. Und insbesondere auch die Zugänglichkeit und Inanspruchnahme grundlegender privater und öffentlicher Dienste und Leistungen, wozu im Allgemeinen auch die Versicherungsbranche zu zählen ist. Der Anhang III soll regelmäßig überprüft und kann durch die EU-Kommission durch einen delegierten Rechtsakt geändert werden (Artikel 7).

Hier gibt es jedoch noch Einschränkungen, denn in Artikel 6 (3) werden wiederum Kriterien dafür genannt, wann eine KI-Anwendung nicht also hochriskant gilt, obwohl sie in die, in Anhang III genannten Bereiche fällt. Dies ist etwa dann der Fall, „wenn es kein erhebliches Risiko der Beeinträchtigung in Bezug auf die Gesundheit, Sicherheit oder Grundrechte natürlicher Personen birgt, indem es unter anderem nicht das Ergebnis der Entscheidungsfindung wesentlich beeinflusst." Speziell gilt dies unter anderem auch, wenn das KI-System dazu bestimmt ist, „das Ergebnis einer zuvor abgeschlossenen menschlichen Tätigkeit zu verbessern" oder „eine vorbereitende Aufgabe für eine Bewertung durchzuführen, die für die Zwecke, der in Anhang III aufgeführten Anwendungsfälle relevant ist."

Die ethischen Leitlinien sowie die Kernanforderungen an eine vertrauenswürdige KI, wie sie die HLEG formuliert hat [2], finden sich zu weiten Teilen in den Anforderungen des AI Acts an Hochrisikosysteme wieder. Die regulatorischen Anforderungen an Hochrisikosysteme nehmen einen

[^0]
[^0]:    ${ }^{4}$ „Bevollmächtigter [...] ist eine in der Union ansässige oder niedergelassene natürliche oder juristische Person, die vom Anbieter eines KI-Systems oder eines KI-Modells mit allgemeinem Verwendungszweck schriftlich dazu bevollmächtigt wurde und sich damit einverstanden erklärt hat, in seinem Namen die in dieser Verordnung festgelegten Pflichten zu erfüllen bzw. Verfahren durchzuführen;" (Artikel 3)

## Page 9
breiten Raum in der KI-Verordnung ein. Dabei wird zwischen Anwendern und Anbietern von KISystemen unterschieden. Beiden Gruppen sind die Artikel 8 bis 15 gewidmet.

Für Hochrisikosysteme muss unter anderem ein Risikomanagementsystem eingeführt werden, welches die Risiken im Zusammenhang mit KI-Systemen während ihres gesamten Lebenszyklus identifizieren und minimieren soll (Artikel 9). Für Hochrisikosysteme gelten auch strenge Regeln, was die Verwendung von Trainings-, Validierungs- und Testdaten angeht. So müssen diese relevant, hinreichend repräsentativ und im Hinblick auf den beabsichtigten Zweck so weit wie möglich fehlerfrei und vollständig sein (Artikel 10). Auch die Dokumentations- und Aufzeichnungspflichten (Artikel 11, 12) sind für Hochrisikosysteme deutlich umfangreicher. Der Anhang IV der Verordnung gibt vor, wie die technische Dokumentation dieser Systeme ausgestaltet sein muss. Viele der Anforderungen decken sich bereits mit der aus anderen Regularien (bspw. der VAIT) geforderten technischen Dokumentation (beispielsweise eine Beschreibung der Systemarchitektur o.ä.), einiges geht aber auch darüber hinaus (beispielsweise die Beschreibung der Trainingsdatensätze).

Wichtig sind auch die Anforderungen an Transparenz und die Möglichkeit der menschlichen Aufsicht. Nach Artikel 13 sind alle Hochrisikosysteme hinreichend transparent zu gestalten und mit einer Gebrauchsanweisung zu versehen. Darüber hinaus müssen sie, gemäß Artikel 14, so konzipiert werden, dass sie während ihres Einsatzes von natürlichen Personen wirksam überwacht werden können. Insbesondere müssen die Systeme übersteuerbar und stoppbar sein. Schließlich werden hohe Anforderungen hinsichtlich Genauigkeit, Robustheit und Cybersicherheit während des gesamten Lebenszyklus einer Hochrisikoanwendung formuliert (Artikel 15).

Für Anbieter von Hochrisikosystemen gelten darüber hinaus weitere Anforderungen, die in den Artikeln 16 bis 27 formuliert sind. Darin geht es unter anderem um die Einführung eines Qualitätsmanagementsystems, um die Pflicht zur Erstellung der technischen Dokumentation, um die Konformitätsbewertung, sowie um automatisch erzeugte Protokolle, Korrekturmaßnahmen, Informationspflichten und ebenso um die Zusammenarbeit mit den zuständigen Behörden.

# 2.2.3. KI-Systeme mit besonderen Transparenzanforderungen 

In die Kategorie drei fallen KI-Anwendungen, die keine Hochrisikosysteme im Sinne von Artikel 6 sind, für die jedoch besondere Transparenzanforderungen formuliert werden.

Dies betrifft unter anderem KI-Systeme, die mit natürlichen Personen interagieren (zum Beispiel Chatbots), sowie die Verwendung von Emotionserkennungssystemen oder Systemen zur biometrischen Kategorisierung. In diesen Fällen muss die betroffene Person darüber informiert werden.

Beim Einsatz von KI-Systemen, die Bild-, Ton- oder Videoinhalte erzeugen oder manipulieren, die wirklichen Personen, Gegenständen, Orten oder anderen Einrichtungen oder Ereignissen merklich ähneln und einer Person fälschlicherweise als echt oder wahrhaftig erscheinen könnten („Deepfake"), muss offengelegt werden, dass die Inhalte künstlich erzeugt oder manipuliert wurden.

Während des langwierigen Gesetzgebungsverfahrens wurde der Gesetzgeber von der Realität in Form der Large Language Modelle überholt. Spätestens seit dem breiten öffentlichen Interesse an ChatGPT im Jahr 2023 wurde klar, dass auch generative KI bei der Regulierung berücksichtigt sein sollte. Tatsächlich wäre der AI Act an der Kontroverse, wie weit die Regulierung hier gehen sollte, auf den letzten Metern fast noch gescheitert.

Zwischen der Einstufung generativer KI als Hochrisiko und keine Regulierung einigte man sich quasi „in der Mitte" und formulierte neue Transparenzanforderungen. Dabei werden die Basismodelle in zwei Kategorien eingeteilt, in „KI-Modelle mit allgemeinem Verwendungszweck" (generalpurpose AI models) und „KI-Modelle mit allgemeinem Verwendungszweck mit systemischem Risiko" (general-purpose AI models with systemic risk). Die Begriffe sind dabei so allgemein formuliert, dass sie sowohl die verbreiteten Transformer-Modelle wie GPT-4, DALL-E, BERT, etc., als auch künftige (Weiter-) Entwicklungen umfassen.

## Page 10
Kriterien zur Klassifizierung, also wann von einem systemischen Risiko auszugehen ist, werden in Artikel 51 sowie im Anhang XIII formuliert. Zentral sind dabei unter anderem die Komplexität des Modells, die Größe der verwendeten Datensätze, die Menge der für das Trainieren des Modells verwendeten Berechnungen, sowie die Zahl der registrierten Endnutzer.

Für die Anbieter von „KI-Modellen mit allgemeinem Verwendungszweck" gelten umfängliche Transparenzanforderungen. Hier sind unter anderem die Pflicht zur Erstellung und laufenden Aktualisierung einer technischen Dokumentation zu nennen. Inhalt und Umfang dieser technischen Dokumentation sind in Anhang XI formuliert. Darüber hinaus müssen Anbieter über eine Strategie zur Einhaltung des Urheberrechts der EU sowie über eine hinreichend detaillierte Zusammenfassung der für das Training des KI-Modells mit allgemeinem Verwendungszweck verwendeten Inhalte verfügen (Artikel 53). Aber auch hier gibt es Ausnahmen, denn diese Pflichten ,... gelten nicht für Anbieter von KI-Modellen, die im Rahmen einer freien und quelloffenen Lizenz bereitgestellt werden ..." (Artikel 53 (2)).

Für die Anbieter von „KI-Modellen mit allgemeinem Verwendungszweck mit systemischem Risiko" gelten darüber hinaus weitergehende Anforderungen. Gefordert sind hier unter anderem eine Modellbewertung mit standardisierten Protokollen und Instrumenten, die dem Stand der Technik entsprechen. Zudem müssen Anbieter mögliche systemische Risiken identifizieren, bewerten und mindern; einschlägige Informationen über schwerwiegende Vorfälle und mögliche Abhilfemaßnahmen erfassen und dokumentieren und das Büro für Künstliche Intelligenz und gegebenenfalls die zuständigen nationalen Behörden unverzüglich darüber unterrichten, sowie ein angemessenes Maß an Cybersicherheit gewährleisten (Artikel 55).

# 2.2.4. Alle weiteren KI-Anwendungen 

Für KI-Anwendungen, die in keine der obigen Kategorien fallen, also weder verbotene oder Hochrisikosysteme sind noch zu jenen mit besonderen Transparenzanforderungen gehören, ist keine Regulierung vorgesehen.

In diesem Fall wird eine Selbstregulierung mit Hilfe sogenannter Verhaltenskodizes empfohlen (Artikel 95). Diese können Unternehmensindividuell sein oder Branchenweit gelten.

### 2.3. Sanktionen

Welche Bedeutung die Regulierung von Künstlicher Intelligenz und damit die Einhaltung der Vorschriften des AI Acts hat, verdeutlichen nicht zuletzt die in Artikel 99 festgelegten möglichen Sanktionen bei Nichtbeachtung des Gesetzes. Ähnlich wie bei der Datenschutzgrundverordnung können diese mitunter recht hoch ausfallen. Wer beispielsweise eine nach Artikel 5 nicht erlaubte Praktik betreibt, kann mit einer Geldstrafe von bis zu 35 Mio. € oder $7 \%$ des weltweiten Jahresumsatzes belegt werden, je nachdem welches von beidem der höhere Betrag ist. Auch die Nichteinhaltung der weiteren oben genannten Maßnahmen kann mit hohen Geldstrafen einhergehen.

Für Versicherungsunternehmen ist das Vertrauen ihrer Kunden zentral für den Unternehmenserfolg. Somit sind neben Sanktionen seitens des Gesetzgebers insbesondere auch Reputationsschäden, die mit der Verwendung von KI-Systemen einhergehen könnten, zu vermeiden. Auch aus dieser Perspektive sind die Einhaltung der KI-Vorschriften und das Sicherstellen eines sicheren und ethischen Einsatzes von KI von großer Bedeutung.

### 2.4. Inkraftreten und Geltungsbeginn

Gemäß Artikel 113 trat die Verordnung am zwanzigsten Tag nach ihrer Veröffentlichung im Amtsblatt der Europäischen Union in Kraft, also am 01. August 2024.

Sie gilt ab dem 2. August 2026.
Einzelne Anforderungen gelten jedoch zu abweichenden Terminen:

## Page 11
a) Die Kapitel I und II gelten seit dem 2. Februar 2025. Dies betrifft die allgemeinen Bestimmungen und insbesondere die verbotenen Systeme.
b) Kapitel III Abschnitt 4, Kapitel V, Kapitel VII und Kapitel XII sowie Artikel 78 gelten ab dem 2. August 2025, mit Ausnahme des Artikels 101. Diese betreffen im Wesentlichen behördliche Maßnahmen, wie etwa die Schaffung notifizierender Behörden, sowie die Governance auf Unionsebene und Transparenzanforderungen für KI-Modelle mit allgemeinem Verwendungszweck.
c) Artikel 6 Absatz 1 und die entsprechenden Pflichten gemäß dieser Verordnung gelten ab dem 2. August 2027. Dies betrifft die weiteren, über den Anhang III hinausgehenden Einstufungsvorschriften für Hochrisikosysteme.

Diese Verordnung ist in allen ihren Teilen verbindlich und gilt unmittelbar in jedem Mitgliedstaat der Europäischen Union.

# 3. Auswirkungen 

### 3.1. Auswirkungen auf Versicherungen

Die Frage, welche Anwendung im Versicherungsumfeld, welcher Regulatorik unterliegt, lässt sich in drei Schritten beantworten. Zunächst ist zu prüfen, ob die Anwendung als KI im Sinne der Verordnung anzusehen ist. Im zweiten Schritt ist zu prüfen, in welche der Kategorien die KI-Anwendung fällt. Daraus lassen sich dann die Anforderungen an den Anwender sowie den Anbieter ableiten.

Ein Überblick über die Auswirkungen des AI Acts auf Versicherungen lässt sich aus der Betrachtung der Kategorien gewinnen.

Verbotene Systeme kommen in der Versicherungsbranche praktisch nicht vor. Diesbezüglich besteht hier also kein Handlungsbedarf.

Hochrisikosysteme werden in Art. 6, Absatz 2 definiert. In Anhang III werden verschiedene Anwendungsfelder aufgelistet. Im fünften Anwendungsfeld „Zugänglichkeit und Inanspruchnahme grundlegender privater und öffentlicher Dienste und Leistungen" werden auch die Versicherungen explizit genannt. Konkret heißt es dort:
„KI-Systeme, die bestimmungsgemäß für die Risikobewertung und Preisbildung in Bezug auf natürliche Personen im Fall von Kranken- und Lebensversicherungen verwendet werden sollen;" ([1], Anhang III, Punkt 5c).

Die umfangreichen Anforderungen an Hochrisikosysteme beschränken sich damit zunächst auf die Sparten Kranken- und Lebensversicherungen und hier nur auf wenige spezielle KI-Anwendungen.

Auf Grund der in Artikel 6 (3) formulierten Ausnahmen fällt jedoch nicht jede KI-Anwendung, die an der Risikobewertung oder der Preisbildung in der Kranken- und Lebensversicherung beteiligt ist, in die Kategorie der Hochrisikoanwendungen. ${ }^{5}$

KI-Systeme mit besonderen Transparenzanforderungen begegnen uns in der Versicherung beim Einsatz von Chatbots in der Kundenkommunikation. Hier muss der Nutzer darüber informiert werden, dass er mit einer KI interagiert.

[^0]
[^0]:    ${ }^{5}$ Der Vollständigkeit halber sei erwähnt, dass gemäß [1], Anhang III, Punkt 4 auch der Einsatz von KI im Personalmanagement in Unternehmen und damit insbesondere auch in Versicherungen als Hochrisikoanwendung klassifiziert ist.

## Page 12
Für vielfältige Anwendungen, etwa im Wissensmanagement, werden aktuell im Versicherungsumfeld Large Language Modelle angewandt. In der Praxis werden dafür keine eigenen Basismodelle verwendet, sondern es wird auf den vorhandenen und verfügbaren Basismodellen aufgesetzt, die als KI-Modelle mit allgemeinem Verwendungszweck angesehen werden. In diesem Kontext sind Versicherungsunternehmen von zwei Seiten von den Transparenzpflichten betroffen: Einerseits als Adressaten der Offenlegungspflichten der Anbieter von Basismodellen, die sie verwenden. Zum anderen selbst als Anbieter von spezialisierten Sprachmodellen.

Die Vielzahl weiterer Anwendungen im Versicherungsumfeld fallen hingegen in die Kategorie vier jener Anwendungen, für die keine zusätzliche Regulierung vorgesehen ist. Hier besteht die Möglichkeit einer freiwilligen Selbstverpflichtung, wovon auf Grund der bereits bestehenden umfänglichen Regulierung in der Versicherungsbranche aktuell nicht auszugehen ist.

Zusammenfassend stellen wir fest, dass Versicherungen von der KI-Regulierung in überschaubarer (moderater) Weise betroffen sind, insbesondere:

- KI-Anwendungen für Risikobewertung und Preisbildung in Bezug auf natürliche Personen im Fall von Kranken- und Lebensversicherungen können Hochrisikoanwendungen, und damit umfassend reguliert sein.
- KI-basierte Kommunikation (Chatbots) mit natürlichen Personen unterliegt der Mitteilungspflicht, dass mit einer KI interagiert wird.

Für alle weiteren KI-Anwendungen sind Selbstregulierung mit Hilfe sogenannter Verhaltenskodizes möglich. Angesichts der bereits bestehenden umfänglichen Regulierung in der Versicherungsbranche sind kurzfristig jedoch keine diesbezüglichen Initiativen zu erwarten.

# 3.2. Auswirkungen für Aktuarinnen und Aktuare 

Aktuarinnen und Aktuare, die in KI-Projekten tätig sind, spielen hinsichtlich der Regulatorik künftig eine besondere Rolle. Auf Grund ihrer spezifischen Ausbildung, ihrer Erfahrung und Kompetenz sind sie in der Lage, zu analysieren, welche regulatorischen Anforderung für eine KI-Anwendung gelten und wie diese effizient umzusetzen sind. Dies betrifft insbesondere komplexe und anspruchsvolle Anforderungen, wie Transparenz, Fairness oder Erklärbarkeit.

Andererseits stehen Aktuarinnen und Aktuare in der Pflicht, sich diesbezüglich weiterzubilden und stets auf dem aktuellen Stand, sowohl der technologischen Entwicklung als auch des regulatorischen Umfelds zu sein.

Neben den umfänglichen Anforderungen an KI-Systeme werden im AI Act auch Anforderungen an die KI-Kompetenz der Beteiligten formuliert. In Artikel 4 heißt es dazu: „Die Anbieter und Betreiber von KI-Systemen ergreifen Maßnahmen, um [...] sicherzustellen, dass ihr Personal [...] über ein ausreichendes Maß an KI-Kompetenz verfügen, wobei ihre technischen Kenntnisse, ihre Erfahrung, ihre Ausbildung und Schulung und der Kontext, in dem die KI-Systeme eingesetzt werden sollen, sowie die Personen oder Personengruppen, bei denen die KI-Systeme eingesetzt werden sollen, zu berücksichtigen sind."

Dies betrifft tatsächlich alle Personen, beginnend mit jenen, die KI-Anwendungen entwickeln bis hin zu jenen Personen, die dies anwenden.

Vor diesem Hintergrund sind Aktuarinnen und Aktuare geradezu prädestiniert, zentrale Rollen in KI-Projekten einzunehmen und somit einen bedeutenden Beitrag zu einem sicheren, transparenten und ethischen Einsatz von KI zu leisten.

## Page 13
Sollte es für Aktuarinnen oder Aktuare nötig werden, eine Risikobewertung einer KI-Anwendung durchzuführen, so findet sich untenstehend eine konkrete Checkliste, welche Schritt für Schritt durchgegangen werden kann. Es handelt sich dabei um einige zentrale Fragen, welche darüber entscheiden in welchem Maße die Anwendung reguliert wird. Im Prinzip geht es hierbei darum, ob es sich um ein Hochrisikosystem handelt oder nicht.

Checkliste:
Gegeben sei eine konkreter KI-Anwendung. Zur Ermittlung der konkreten Anforderungen gemäß der KI-Verordnung kann folgendermaßen vorgegangen werden.
(1) Fällt die Anwendung in den Anwendungsbereich gemäß Artikel 2? Konkret: Wird die Anwendung in der der Europäischen Union (EU) in Verkehr gebracht oder hat sie Nutzer in der EU oder werden die Ergebnisse in der EU verwendet?

Falls nein: Die KI-Verordnung ist nicht anwendbar. -> fertig
Falls ja:
(2) Handelt es sich um eine KI-Anwendung im Sinne der KI-Verordnung? Konkret: Ist die Definition für ein KI-System in Artikel 3 erfüllt?

Falls nein: Die KI-Verordnung ist nicht anwendbar. -> fertig
Falls ja:
(3) Ist eines der Kriterien aus Artikel 5 für verbotene Anwendungen erfüllt?

Falls ja: Bei der KI-Anwendung handelt es sich um ein verbotenes System, das sofort abgeschaltet werden muss. -> fertig
Falls nein:
(4) Ist eines der Kriterien aus Artikel 6 erfüllt? Konkret: Fällt die KI-Anwendung in eine der Kategorien in Anhang III?

Falls nein: -> Weiter mit Schritt (7).
Falls ja:
(5) Trifft eine der Ausnahmeregelungen gemäß Artikel 6 (3) zu? Konkret: Besteht kein erhebliches Risiko der Beeinträchtigung in Bezug auf die Gesundheit, Sicherheit oder Grundrechte natürlicher Personen, indem es unter anderem nicht das Ergebnis der Entscheidungsfindung wesentlich beeinflusst? Oder: Ist das KI-System dazu bestimmt, das Ergebnis einer zuvor abgeschlossenen menschlichen Tätigkeit zu verbessern oder eine vorbereitende Aufgabe für eine Bewertung durchzuführen, die für die Zwecke, der in Anhang III aufgeführten Anwendungsfälle relevant ist?

Falls ja: Die KI-Anwendung wird nicht als Hochrisikoanwendung klassifiziert. -> Weiter mit Schritt (7).
Falls nein:
(6) Sind wir nur Anwender der KI-Anwendung (also nicht Entwickler, Anbieter oder Betreiber)?

Falls ja: Die KI-Anwendung wird als Hochrisikoanwendung klassifiziert. Als Anwender müssen wir die Anforderungen für die Anwendung von Hochrisikosystemen erfüllen, wie sie in den Artikeln 8 bis 15 (u.a. Risikomanagement, Daten und Daten-Governance, Technische Dokumentation, Aufzeichnungspflichten, Transparenz und Bereitstellung von Informationen, Menschliche Aufsicht, Genauigkeit, Robustheit und Cybersicherheit) definiert sind. -> fertig

Falls nein: Die KI-Anwendung wird als Hochrisikoanwendung klassifiziert. Als Anbieter, Betreiber und/oder Entwickler müssen wir zusätzlich zu den Anforderungen aus den Artikel 8 bis

## Page 14
15 auch die Anforderungen aus den Artikeln 16 bis 21 (u.a. Qualitätsmanagementsystem, Aufbewahrung der Dokumentation, automatisch erzeugte Protokolle, Korrekturmaßnahmen und Informationspflicht, Zusammenarbeit mit den zuständigen Behörden) sowie 72 (Beobachtung nach dem Inverkehrbringen) und 73 (Meldung schwerwiegender Vorfälle) erfüllen. Darüber hinaus sind wir verpflichtet, eine Komformitätsbewertung gemäß Artikel 43 auf Basis der Erläuterungen in Anhang VI vorzunehmen und eine Konformitätserklärung gemäß Artikel 47 auf Basis der Anforderungen gemäß Anhang V abzugeben. Zudem besteht die Pflicht der Registrierung gemäß Artikel 49 in der EU-Datenbank für Hochrisikosysteme (Artikel 71). -> fertig
(7) Erfüllt die KI-Anwendung eines der Kriterien für Anwendungen mit besonderen Transparenzpflichten gemäß Artikel 50 (Interaktion mit natürlichen Personen, Emotionserkennungssysteme, Deepfake)?

Falls ja: Die Transparenzanforderungen gemäß Artikel 50 sind zu beachten. -> fertig Falls nein:
(8) Stellt der Anbieter die KI-Anwendung im Rahmen einer freien und quelloffenen Lizenz bereit?

Falls Ja: Die Anforderungen gemäß Artikel 53 gelten hier nicht. -> weiter mit Schritt (11) Falls nein:
(9) Fällt die KI-Anwendung in die Kategorie „KI-Modelle mit allgemeinem Verwendungszweck" (general-purpose AI models) gemäß Artikel 51?

Falls nein: -> Weiter mit Schritt (11).
Falls ja:
(10) Fällt die KI-Anwendung in die Kategorie „KI-Modelle mit allgemeinem Verwendungszweck mit systemischem Risiko" (general-purpose AI models with systemic risk) gemäß Artikel 51 und Anhang XIII?

Falls nein: Die Anforderungen an KI-Modelle mit allgemeinem Verwendungszweck (ohne systemisches Risiko) gemäß Artikel 53 sind zu beachten (u.a. technische Dokumentation, Informationen über Fähigkeiten und Grenzen des KI-Modells, Strategie zur Einhaltung des Urheberrechts, Informationen über die für das Training verwendeten Inhalte).

Falls ja: Zusätzlich zu den Anforderungen an KI-Modelle mit allgemeinem Verwendungszweck gemäß Artikel 53 sind die Anforderungen an KI-Modelle mit allgemeinem Verwendungszweck mit systemischem Risiko gemäß Artikel 55 zu beachten (u.a. Modellbewertung mit standardisierten Protokollen, Bewertung systemischer Risiken, Informationen über schwerwiegende Vorfälle, Cybersicherheit).
(11) Existieren Verhaltenskodizes oder Selbstverpflichtungen für KI-Anwendung in der Branche oder dem Unternehmen?

Falls ja: Diese Verhaltenskodizes oder Selbstverpflichtungen sind zu beachten. -> fertig
Falls nein: Es bestehen keine zusätzlichen Anforderungen. -> fertig

## Page 15
# 4. Literaturverzeichnis 

[1] VERORDNUNG (EU) 2024/1689 DES EUROPÄISCHEN PARLAMENTS UND DES RATES vom 13. Juni 2024 zur Festlegung harmonisierter Vorschriften für künstliche Intelligenz und zur Änderung der Verordnungen (EG) Nr. 300/2008, (EU) Nr. 167/2013, (EU) Nr. 168/2013, (EU) 2018/858, (EU) 2018/1139 und (EU) 2019/2144 sowie der Richtlinien 2014/90/EU, (EU) 2016/797 und (EU) 2020/1828 (Verordnung über künstliche Intelligenz)
[2] ETHICS GUIDELINES FOR TRUSTWORTHY AI, High-Level Expert Group on Artificial Intelligence (HLEG), European Commission, Brüssel, 08.04.2019