# 20250324-SOA-ai-bulletin

## Page 1
# ACTUARIAL INTELLIGENCE BULLETIN 

March 2025

## TABLE OF CONTENTS

Charting the Future: SOA's Strategic Focus on Artificial Intelligence ..... 2
SOA PD Edge+ Artificial Intelligence Segments. ..... 3
NAIC Updates ..... 5
Tips for Prompting ..... 7
Advancing AI Safety at the Society of Actuaries Research Institute ..... 8
Open vs. Closed LLMs ..... 10
AI Insights 2025 ..... 12
Editorial Committee ..... 12
About the Society of Actuaries Research Institute ..... 13

Welcome to the March 2025 edition of the SOA Research Institute AI Bulletin! This bulletin serves as a platform for sharing knowledge and fostering collaboration around artificial intelligence within the actuarial community. Explore articles on strategic initiatives, practical tips, and research advancements, all aimed at empowering actuaries to leverage AI responsibly and effectively.

## Page 2
# Charting the Future: SOA's Strategic Focus on Artificial Intelligence 

## RYAN SILHAVY

As artificial intelligence (AI) continues to drive new forms of value and risk, the SOA has built a strategy to embrace the future of AI innovation, skill development, and responsible implementation. In 2024, the SOA Board of Directors adopted strategic principles and an actionable roadmap that will guide the organization's AI initiatives through 2025 and beyond.

Development of the strategy started with a basic question: "Is it critical for the SOA to engage with artificial intelligence?" The question is simple, but the implications are weighty; we believe AI has the potential to fundamentally reshape actuarial work, perhaps even more profoundly than the evolution of pencil and paper to personal computing. We cannot be certain of what the future will hold five or ten years down the road, but it is incumbent on the SOA to be prepared for whatever future state emerges. With this perspective, we developed two strategic principles to guide the SOA's approach to artificial intelligence.

The first strategic principle focuses on promoting and offering AI skill development to candidates and members. Future actuaries will need to be proficient in both traditional actuarial science concepts and AI capabilities and applications. Actuaries will need support with implementation, regulatory considerations, and governance. This principle aims to create "AI-enhanced actuaries" who can effectively apply artificial intelligence in their work, maintain ethical standards, and drive new forms of value for the stakeholders and industries we serve.
"Integration of AI into actuarial practice represents a natural evolution of the profession," said Ron Richman, an actuary at Old Mutual Insure, during a recent International Actuarial Association (IAA) conference. "By embracing AI and developing new skills, actuaries can unlock opportunities for innovation, efficiency, and value creation, both within the insurance industry and beyond."

We are already working toward this end; AI content is being integrated across all learning channels, including PD Edge+, webinars, major meetings, and symposiums. The content covers topics such as model explainability, bias mitigation, and responsible AI implementation. This comprehensive approach ensures that practicing actuaries can effectively incorporate AI tools into their work while maintaining professional standards.

The second strategic principle centers on building a robust network of AI expertise to inform, support, and strengthen the profession. The SOA is engaging with AI innovators, regulators, academia, and governmental organizations to gather thought leadership and cutting-edge ideas, while promoting innovation in actuarial applications. This will ensure our members, candidates, and stakeholders are ready to meet technological changes in the immediate term and understand how the profession may change over the long term.

The SOA Research Institute has expanded its partnerships with universities, technology firms, and cross-functional governmental organizations. Notable collaboration includes work with the (U.S.) National Association of Insurance Commissioners (NAIC) on developing frameworks for the responsible use of AI in insurance applications. The SOA is also investing in research focused on next generation AI technologies that will be incorporated into actuarial work. This research will support the profession by developing frameworks for ethical implementation, ensuring that new technologies align with actuarial principles, and providing best practices around model selection, use, and limitations.

Finally, we are also using AI to drive operational improvements; we are leveraging AI tools to automate routine work, create efficiencies, and enhance the experience of SOA stakeholders. We will continue to invest in AI tools that improve business operations, under a governance structure that mitigates potential risks. We seek to improve

## Page 3
the experience of candidates, members, and other stakeholders through intelligent personalization, reducing friction, and scaling services that were challenging to scale before AI.

As the SOA moves forward with these strategic initiatives, the focus remains on maintaining the profession's core commitment to ethical and responsible standards while embracing technological advancement. Through careful implementation of these principles, the SOA will support its members through changes and champion opportunities for actuaries to drive innovation and new forms of value for the organizations they serve.

Ryan Silhavy is Director of Innovation for the SOA.

# ACTUARIAL <br> INTELLIGENCE BULLETIN 

## SOA PD Edge+ Artificial Intelligence Segments JON FORSTER

The Society of Actuaries (SOA) launched a new professional development product, Professional Development Edge+ (PD Edge+), at the 2024 ImpACT Conference to facilitate quality continuing education for actuaries. This product will focus heavily on AI training in 2025.

PD Edge+ is a subscription-based product with high-quality and dynamic professional development content, including:

## Live Webcasts

The SOA produces approximately 70 webcasts each year. Each live broadcast runs 60-90 minutes and covers actuaries' core competencies presented by experts in each topic.

## Recordings of Live Webcasts

Live webcasts are available five business days after they have aired, in addition to having access to 500+ webcast recordings from the past three years.

## Recordings of Live Conferences

There are 600+ recordings of live sessions from the SOA's major meetings (ImpACT, Health, and Life/ValAct) held over the past three years covering various topics. Content from major meetings will be available six months after the live event concludes.

## NEW Learning Tracks

PD Edge+ features seven Learning Tracks that cover key actuarial topics, including Artificial Intelligence, Influential Leadership, Investment, Life Insurance, Long-Term Care, Medicare, and Retirement. The courses within the learning tracks are designed with learning objectives, summaries, audio and video clips, and interactive knowledge check questions.

## Page 4
# Short-form Professional Development Videos: 

These short-form videos ( 5 to 15 minutes) align with the new learning tracks. A team of experts creates these standalone videos, which offer essential information to actuaries.

The AI track within PD Edge+ offers a course entitled:
AI Dynamics: Evolution and Implementation: This is the first AI course on the platform and is meant to provide users with an overview of AI and give them a base on how to leverage AI in actuarial work. We plan to add more courses quarterly in 2025.

Additionally, there are 19 short-form AI videos that cover a broad variety of topics. Below are a few worth highlighting:

Technology in Artificial Intelligence: Dave Friesen provides a comprehensive overview of the Artificial Intelligence and Data Science learning track. It covers the role of AI in actuarial work, ethical considerations, and the challenges and limitations of AI. The video also discusses the importance of cloud ecosystems, AI tools, and automation trends, emphasizing the evolution of platforms like Python and low-code solutions. Additionally, it highlights the criticality of lifecycle management, governance, and data foundations in AI applications. Dave Friesen | LinkedIn

Challenges and Limitations of AI Data: Joe Long, ASA, MAAA, a consulting actuary and data scientist at Milliman, discusses the challenges and limitations of developing and using machine learning models, emphasizing the critical role of data quality and aggregation. Joe highlights the importance of understanding data nuances and ensuring alignment with business problems before model development. The video also covers the complexities of building and implementing models, adhering to Actuarial Standards of Practice (ASOPs), and addressing ethical implications in the insurance industry. Additionally, it underscores the need for standardization across diverse datasets and the practical challenges of deploying models in real-world scenarios. Finally, this video emphasizes the actuary's role in transforming raw data into reliable models and ensuring fairness and equity in model applications. Joe Long LinkedIn

Primer on Cloud Computing: Arthur da Silva, FSA, FCIA, introduces the core concepts of cloud computing and cloud platforms and explains the benefits and trade-offs of using cloud-native, cloud-enabled, and cloud-based applications. He also provides an overview of the current and future usage of cloud computing among actuaries. The video is aimed at actuarial professionals who want to learn more about the cloud and how it can enhance their work. Arthur da Silva | LinkedIn

Learn more about PD Edge+ or contact Amy Schutzenhofer at ASchutzenhofer@soa.org or Jon Forster at jforster@soa.org.

We plan to continue rolling quality content on this platform in 2025 and beyond.
Jon Forster, ASA, is the Professional Development Learning \& Content Design Director for the SOA.

## ACTUARIAL INTELLIGENCE BULLETIN

## Page 5
# NAIC Updates 

## BY DOROTHY ANDREWS

This is the time of year when the National Association of Insurance Commissioners (NAIC) is busy appointing new chairs and co-chairs of its various committees, taskforces, and workgroups. The letter Committees A through F lead the most important work the NAIC performs. Within the committees, there are working groups and taskforces, and within taskforces, there are various subgroups. The 2025 committee chairs are:

- Executive (EX) Committee - Jon Godfread (ND)
- Life Insurance and Annuities (A) Committee - Judith L. French (OH)
- Health Insurance and Managed Care (B) Committee - Glen Mulready (OK)
- Property and Casualty Insurance (C) Committee - Michael Conway (CO)
- Market Regulation and Consumer Affairs (D) Committee - Dean L. Cameron (UT)
- Financial Condition (E) Committee - Nathan Houdek (WI)
- Financial Regulation Standards and Accreditation (F) Committee - Lori K. Wing-Heier (AK)
- International Insurance Relations (G) Committee - Eric Dunning (NE)
- Innovation, Cybersecurity, and Technology (H) Committee - Barbara Richardson (AZ)

The complete list of appointments for committees, taskforces, and working groups can be found on the NAIC website at: https://content.naic.org/article/naic-announces-2025-committee-leaders

One of the most important projects the NAIC has undertaken in the past few years is the collection of data on the insurance industry's use of artificial intelligence and big data. The NAIC has completed surveys in the following practice areas:

- Private Passenger Automobile Insurance (December 2022)
- Homeowners Insurance (August 2023)
- Life Insurance (December 2023)
- Health Insurance (Publication due March 2025)

The analysis reports were published on the dates indicated above and can be found at:
https://content.naic.org/committees/h/big-data-artificial-intelligence-wg
The surveys represent the most comprehensive data source on how the industry is using artificial intelligence (including machine learning techniques) and big data, and the governance structures they are using to govern the use of these technologies to innovate the conduct of insurance. The NAIC's interest in the insurance industry's use of artificial intelligence is threefold. First, regulators want to understand how the industry is using artificial intelligence in major insurance operational areas of insurance, such as pricing, underwriting, claims, and fraud detection. Regulators also want to understand the types of data, including third-party data, fueling the industry's use of artificial intelligence. Second, regulators want to make sure that the industry's use of artificial intelligence is

## Page 6
innovative and does not raise cause for concern regarding consumer safety and adverse impacts. Third, regulators want to understand the scope of artificial intelligence governance frameworks that are being implemented by insurers, and how closely those frameworks align with the NAIC Artificial Intelligence Principles and the NAIC Artificial Intelligence Model Bulletin ("Bulletin").

The NAIC Artificial Intelligence Principles reflect five guiding principles of a sound AI governance framework. Insurance companies and other parties should ensure their use and implementation of artificial intelligence as:

- Fair and Ethical
- Accountable
- Compliant
- Transparent
- Secure, Safe and Robust

A detailed discussion of the principles can be found on the NAIC website at:
https://content.naic.org/committees/h/big-data-artificial-intelligence-wg.
The NAIC Artificial Intelligence Model Bulletin was adopted in December 2023 by the membership. The goals of the bulletin are to provide guidance to state regulators in their development of a framework by which to evaluate an insurer's use of artificial intelligence and big data in their insurance operations. The Bulletin provides guidance in the following areas:

- Legislative Authority
- Definitions
- Regulatory Guidance and Expectations
- Regulatory Oversight and Examination Considerations

Third-party artificial intelligence systems and data are addressed throughout the Bulletin. The full scope of the Bulletin can be found on the NAIC website at: https://content.naic.org/committees/h/innovation-cybersecurity-technology-cmte.

The Spring National Meeting will be held in Indianapolis this year from March 23 to March 26. There are virtual options to participate as well. Details on the agenda and registration can be found on the NAIC website at: https://content.naic.org/events.

Dorothy L. Andrews, PhD, ASA, MAAA, is the Senior Behavioral Data Scientist and Actuary for the National Association of Insurance Commissioners.

# ACTUARIAL <br> INTELLIGENCE BULLETIN

## Page 7
# Tips for Prompting 

## BY DAN KIM AND DAVE INGRAM

We hope the following Prompt Engineering tips will help you write your prompts like an expert.

1. You can start your prompt by assigning an identity to the LLM and providing the background. "You are an experienced personal trainer of former athletes trying to regain their youthful figure" if you want advice about your workouts. This identity assignment helps the LLM to have a better idea of what sort of answer to give you, how technical and to what level of detail.
2. It also helps if the LLM knows about you. It seems intrusive, but if you think of it, if you do not reveal something about yourself, the LLM will likely give you an answer directed to a generic person, and you probably are not a generic person. The sorts of things that the LLM would find helpful are your professional background, level of experience and expertise, and subject matter interests.
3. ChatGPT loves to give you answers in the form of bullet point lists, but sometimes you don't want that. Just saying "no bullets" doesn't always work. However, it does seem to work better to give a positive statement about the format you do want. I usually do both, saying "Answer in prose with paragraphs and sentences with no bullets."
4. Sometimes, however, I want to look at a high volume of similar data and a table allows me to focus on the comparisons. You can get whichever format you want, but usually you can only get it the first time if you include your desired format in your prompt. Other possible formats include Timelines, Codeblocks and downloadable files. LLM does not handle this part well, at least as of the writing of this article. Please always double-check the answers from LLM.
5. One more thing that you might want to mention as a part of your prompt is an indication of what you are planning to do with the response. Sometimes, I tell the LLM that I am preparing a board presentation, or teaching a class and, if I want a compelling response, I say that I am trying to make a sale. The difference in response is often subtle but real.
6. Usually, I am fine with the tone of voice that the LLM uses to respond to my prompt, but I saw a list of other tones that you could ask for: Critical - looking at both sides of an issue as opposed to the relentless positive attitude that is normal; Persuasive - for when I need to use the response to win someone over; Narrative - a storytelling approach; or Direct - straight to the point, probably what I want in general, just the facts. https://www.nature.com/articles/d41586-024-01042-3
7. It is supposed to help if you specify how you are going to use the response. Often, I just say that I am looking for a blog post. Sometimes I ask for an essay. An essay is more formal and longer than a blog post. Blog posts are likely to be opinions, unsupported by a logical argument, while essays are well reasoned discussions which rely on facts and logic to reach any conclusion.

The above are basics of prompting. You should consider including those ideas with every prompt. The rest of these Tips are things that we have learned either from our own trial and error or from others. If you try something here and find it useful, please pass it along.
8. I will often feed something I have written into the LLM and ask, "Is there anything else that should be considered?" Usually, it will give me three to five additional subtopics. Some of those may be just a different way of saying something that I have already covered. Other points are minor items that I feel totally comfortable leaving out. But many times, there is at least one thing that I left out and should have included, and that makes this step worthwhile. Some LLMs allow tuning the hyperparameter like "temperature," which increases the randomness of the response.
9. Sometimes, I ask the LLM to rewrite something and it just isn't very good. Using "Rewrite" without any modifiers doesn't give any directions. "Streamline" or "Elaborate" would each give a different response. I am thinking that I often want it to "Embellish" or "Enliven." I might tell it to "Soft-pedal" when I should be asking it to "Sensationalize."

## Page 8
10. When I try to ask questions of LLMs, responses are sometimes too generic or less relevant to my intention. It helps me refine and make my questions more specific. This iterative process improves my questionframing skills for interactions with actual people.
11. Requesting a summary of a long article helps you decide if it's worth reading in full. This is useful when facing many articles and needing to prioritize. You can specify the desired summary length, like "Summarize for a one-minute read" or "Summarize in 200 words or less."
12. When seeking edits for your writing, ask to preserve your style and tone. This helps maintain your unique voice and character in the piece.
13. For summaries of actuarial documents, I explicitly say, "Avoid any translation or substitution of actuarial terms in the document" to prevent LLMs from replacing specific terms with more common words, which may not be accurate.
14. Corporate financial report publications are often lengthy and include technical details. You can ask for key market trends, unexpected events, key risks, and takeaways. Also, you can ask if there is any material information that may impact potential investment in the company.
15. Ask for a Strengths, Weaknesses, Opportunities, and Threats (SWOT) analysis, but don't be surprised if you don't get it on the first try; the model often seems lazy and sometimes only covers two or three of the four topics. Don't let it get away with that. Tell it to go back and address all four. Alternately, you can first ask for "Key Points" of a SWOT and then ask the LLM to write a SWOT analysis from those points. That gives you a chance to add in your ideas for additional key points.
16. Many people are concerned about "Hallucinations," meaning the LLM comes back with an answer that sounds good but turns out to be totally made up. One way to check up on the veracity of a response is to ask the LLM for its confidence in a response or the reference for the answer.

With these prompting and AI model usage tips, we hope that you can tune your approach to using LLMs and, with personalization, get more usable responses to your prompts.

Dan Kim, FSA, is Head of Annuities at Talcott Resolution. Dave Ingram, FSA, is a retired risk management actuary and a current member of the SOA Board.

A version of this article appeared previously in the Reinsurance Section Newsletter.

# Advancing AI Safety at the Society of Actuaries Research Institute 

BY R. DALE HALL

In February 2024, the Society of Actuaries (SOA) Research Institute proudly announced its participation in a consortium led by the U.S. Department of Commerce's National Institute of Standards and Technology (NIST). This initiative, known as the U.S. AI Safety Institute Consortium (AISIC), aims to advance the development and deployment of safe, trustworthy artificial intelligence (AI).
![Page 8 Image 1](20250324_soa_ai_bulletin_assets/20250324_soa_ai_bulletin_p08_img1.jpg)

## Page 9
# Background on the U.S. Safety Institute Consortium 

The U.S. Department of Commerce created the formation of the U.S. AI Safety Institute Consortium in late 2023 and the SOA was invited to be an inaugural member of the group early in 2024. This consortium initiative is housed under NIST and brings together over 200 leading AI stakeholders, including AI creators, users, academics, government and industry researchers, and civil society organizations, to ensure the safe and trustworthy development and deployment of AI technologies.

The establishment of the AISIC marked a pivotal moment in AI safety with a collaborative aim to create a platform where experts can share knowledge and data, engage in interdisciplinary research, and develop new guidelines and standards for AI safety.

To achieve its goals, the AISIC has established several working groups, each focusing on specific aspects of AI safety. These working groups will facilitate collaboration among consortium members, enabling them to address key challenges and develop targeted solutions.

- Working Group \#1: Risk Management for Generative AI focuses on developing a companion resource to the AI Risk Management Framework (AI RMF) for generative AI. It aims to create minimum risk management guidance geared toward federal agencies and operationalize the AI RMF.
- Working Group \#2: Synthetic Content will identify existing standards, tools, and methods for authenticating and tracking synthetic content. It will also prevent generative AI from producing harmful content, such as child sexual abuse material or non-consensual intimate imagery.
- Working Group \#3: Capability Evaluations will create guidance and benchmarks for evaluating AI capabilities, focusing on those that could cause harm. It will also ensure the availability of testing environments to support the development of safe AI technologies.
- Working Group \#4: Red-Teaming will establish guidelines for conducting AI red-teaming tests, enabling developers to deploy safe and secure AI systems, particularly for dual-use foundation models.
- Working Group \#5: Safety \& Security will coordinate and develop guidelines related to managing the safety and security of dual-use foundation models, ensuring that societal and technological considerations are incorporated throughout the consortium's activities.


## SOA Research Institute's Role and Contributions

The SOA has formed an advisory group of members and key stakeholders that meet weekly to review current events impacting the growth of AI, provide comments and feedback on AISIC activities and projects, and discuss the evolution of AI risk management and safety frameworks. A primary goal of the SOA is to bring strong actuarial technology and risk management expertise to AISIC, as well as advancing the responsible use of AI through the lens of actuarial profession industries, such as insurance and financial services. These industries often have the consideration of using AI and performing diligent risk management within the context of individual underwriting and risk assessment and with the surrounding regulations to avoid bias and discrimination across protected classes.

A key recent output of AISIC was the release of the document NIST-AI-600-1, Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile on July 26, 2024. This profile assists organizations in identifying unique risks posed by generative AI and proposes actions for generative AI risk management that best aligns with organizational goals and priorities. The SOA's involvement has primarily focused on Working Groups 1, 3, and 5, with the largest amount of activity involved in AISIC's Task Force 1.3. Task Force 1.3 focuses on ways to form practice applications of the Gen AI Risk Management Framework and create reporting templates for testing and documentation. An Expert Panel discussion report on the Gen AI Risk Management Framework is expected to be released by the SOA Research Institute in March 2025.

## Page 10
# AI Safety Institutes around the World 

As AI has become mainstream, an increasing number of AI Safety Institutes (AISIs) have been established globally to test, research, collaborate, and provide guidance on AI systems to ensure that governments and industry regulators can safely implement the potential of AI, without redundant work in each individual country. The U.S. and U.K. established the first AISIs and, today, a global network of ten nations is collaborating to foster AI adoption, share information, and advance the AI science field. In addition, these institutes are partnering more with industry and researchers for research and testing. On February 10, 2025, Scale AI and the United States AI Safety Institute (AISI) announced a partnership to develop improved methods to test frontier AI models. This announcement came at a time (as of this writing) where AISIC partners may have had some uncertainty about the future of the consortium amongst a new administration in the U.S. AI safety and responsible use of technology may be growing to be a common place where technology industries and professions, including the actuarial profession, can work well with government partners and regulators for a broader positive societal purpose.

## Conclusion

The Society of Actuaries Research Institute's participation in the U.S. AI Safety Institute Consortium continues an ongoing role for the actuarial profession to be leaders in risk management and in advancing AI safety and trustworthiness. By leveraging its actuarial expertise and collaborating with other leading AI stakeholders, the SOA Research Institute aims to contribute to the development and deployment of safe and responsible AI technologies. The AISIC's comprehensive approach, including knowledge sharing, collaborative research, guideline development, and assessment of AI systems, ensures that AI technologies are developed and deployed in a manner that prioritizes safety, security, and trustworthiness. Through these efforts, the consortium aims to address the complexities of AI and create a safer AI future for society.
R. Dale Hall, FSA, MAAA, CERA, CFA, is Managing Director of Research at the Society of Actuaries Research Institute.

## ACTUARIAL INTELLIGENCE BULLETIN

## Open vs. Closed LLMs

## BY CAESAR BALONA, FASSA

The LLM landscape is experiencing a significant shift towards open models. This trend is primarily driven by the desire for control, benefiting both users and providers. Open LLMs offer users extensive control over various aspects, including usage, deployment, customization, security, and training data preferences. Conversely, closed LLM providers maintain control for monetization purposes, which isn't inherently negative.

Top 3 Open-Source LLMs:

- LLaMA 3.1 (Meta)
- Falcon 180B (Technology Innovation Institute, UAE)
- Deepseek LLM (Deepseek AI)

Top 3 Proprietary (Closed) LLMs:

- GPT-4o (OpenAI)
- Claude 3.5 (Anthropic)
- Gemini 2.0 (Google DeepMind)

## Page 11
Transparency is another key factor favoring open LLMs. Given the importance of ethics and governance in the LLM space, open models provide varying degrees of transparency, offering greater assurance in these areas. It's worth noting that "openness" exists on a spectrum, with some open LLMs not fully disclosing their training data due to various constraints.

In the insurance and financial sectors, increased control and transparency are particularly valuable. When dealing with sensitive data, having more control over the LLM and how it processes and stores information is essential. Open LLMs provide this capability. Furthermore, the added transparency allows for a better understanding of the LLM's output and facilitates clearer explanations of its functioning.

The following lists summarize the benefits of open LLMs:

# Control 

- Usage: Define exact LLM usage, often surpassing limitations of closed-source models.
- Deployment: Deploy on-premises or in secure cloud environments, aligning with specific requirements.
- Customization: Fine-tune LLMs securely on proprietary data, specify system prompts governing behavior, and control the inference process.
- Security: Implement tailored security measures, such as transparent audits and flexible responses to vulnerabilities.
- Training: Choose LLMs trained on datasets aligned with ethical and regulatory standards.
- Privacy: Exercise complete control over processing and storage of sensitive policyholder data.
- Collaboration: Potential to work with LLM creators to modify functionality for specific needs.


## Transparency

- Ethics: Gain insight into training processes and data, facilitating alignment with ethical standards in insurance.
- Governance: Enhanced transparency may simplify governance procedures.
- Explainability: Better understand LLMs to explain output generation and decision-making processes.
- Bias: Access to model weights and architecture allows deeper control to reduce bias, discrimination, or unfairness in output.
- Trust: All aforementioned factors contribute to building trust in the LLM and its applications.


## Drawbacks of Open LLMs

Despite numerous benefits, open LLMs also have limitations. The primary drawback is the performance gap. Due to reduced monetization, open LLMs often lack the resources available to closed models, leading to consistently lower performance. However, this gap has been narrowing over time, and some companies, like Meta, provide open LLMs (albeit with limited openness) trained with substantial resources. In many cases, this performance difference shouldn't be a limiting factor, as only the most complex use cases require the most powerful LLMs.

Another challenge is the required expertise. Using an open LLM typically involves self-deployment, necessitating technical know-how for deployment and maintenance. This is closely linked to the resource intensity of running all but the smallest LLMs, requiring significant computational power. Consequently, the total cost of using open LLMs may exceed that of closed LLMs when all factors are considered.

Open LLMs often lag behind in offering the latest features and support compared to closed LLMs. With greater resources, closed providers are frequently first movers in newer technologies or training paradigms, offering more features such as code execution and structured output.

## Page 12
*Caesar Balona is a Fellow of the Actuarial Society of South Africa (FASSA) and is Senior Manager: Catastrophe and Climate Change Modelling at Old Mutual Insure in Johannesburg.*

This article is an excerpt from the January, 2025 SOA Research Institute report "Operationalizing LLMs, A guide for Actuaries."


## AI Insights 2025

**BY ROBERT EATON**

The Society of Actuaries will host the 2nd AI Insights for Actuaries on May 14-15, 2025! This is a virtual symposium aimed at providing actionable technical and managerial strategy for implementing AI in your actuarial work. You can find more information on the 1st Symposium (held November 2024) here.


## Editorial Committee

- Jon Forster, ASA
- Dave Ingram, FSA
- Ronald Poon Affat, FSA
- Frank Quan, PhD
- Darryl Wagner, FSA

Thank you for reading the March 2025 SOA Research Institute AI Bulletin. We hope you found these insights valuable. Stay tuned for future editions as we continue to explore the evolving landscape of AI and its impact on the actuarial profession. We encourage you to engage with the SOA Research Institute and share your own experiences and perspectives on AI. For questions, comments, and article submissions, contact rosenalfat@soa.org.
![Page 12 Image 1](20250324_soa_ai_bulletin_assets/20250324_soa_ai_bulletin_p12_img1.jpg)
![Page 12 Image 2](20250324_soa_ai_bulletin_assets/20250324_soa_ai_bulletin_p12_img2.jpg)
![Page 12 Image 3](20250324_soa_ai_bulletin_assets/20250324_soa_ai_bulletin_p12_img3.jpg)

## Page 13
# About the Society of Actuaries Research Institute 

Serving as the research arm of the Society of Actuaries (SOA), the SOA Research Institute provides objective, datadriven research bringing together tried and true practices and future-focused approaches to address societal challenges and your business needs. The Institute provides trusted knowledge, extensive experience and new technologies to help effectively identify, predict and manage risks.

Representing the thousands of actuaries who help conduct critical research, the SOA Research Institute provides clarity and solutions on risks and societal challenges. The Institute connects actuaries, academics, employers, the insurance industry, regulators, research partners, foundations and research institutions, sponsors, and nongovernmental organizations, building an effective network which provides support, knowledge, and expertise regarding the management of risk to benefit the industry and the public.

Managed by experienced actuaries and research experts from a broad range of industries, the SOA Research Institute creates, funds, develops, and distributes research to elevate actuaries as leaders in measuring and managing risk. These efforts include studies, essay collections, webcasts, research papers, survey reports, and original research on topics impacting society.

Harnessing its peer-reviewed research, leading-edge technologies, new data tools and innovative practices, the Institute seeks to understand the underlying causes of risk and the possible outcomes. The Institute develops objective research spanning a variety of topics with its strategic research programs: aging and retirement; actuarial innovation and technology; mortality and longevity; diversity, equity and inclusion; health care cost trends; and catastrophe and climate risk. The Institute has a large volume of topical research available, including an expanding collection of international and market-specific research, experience studies, models and timely research.